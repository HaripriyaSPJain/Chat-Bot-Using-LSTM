{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7duNOFiJRNVX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bV1srZXRUr8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CfIdk4uRXM9",
        "outputId": "c3cac26f-b8b1-46b4-ea05-d852a61f481d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "CnEVlt1eRg4b",
        "outputId": "52940cb4-1775-4d0a-82e8-d7da704bb5c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a07e7ca7-02e3-4d5e-8884-d307c2da97ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a07e7ca7-02e3-4d5e-8884-d307c2da97ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle (2).json': b'{\"username\":\"anandannapurgv\",\"key\":\"118a86403f69fa6ac83b26f713511289\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGyP39WISoOD"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m96uGdkStR8"
      },
      "outputs": [],
      "source": [
        "\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3cmh066SvG4",
        "outputId": "0e95a71b-f03e-4aa8-f84e-4fef034108d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!ls ~/.kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZSh5QvSw4r"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRf8w62OS5_o",
        "outputId": "6eac3c29-b66b-4b63-b1dc-d0e60ba46e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatterbotenglish.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!kaggle datasets download -d kausr25/chatterbotenglish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfTpqQ8qUvae",
        "outputId": "c82ed492-85eb-41c7-df16-195a157c38cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chatterbotenglish.zip\n",
            "replace ai.yml? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ai.yml                  \n",
            "  inflating: botprofile.yml          \n",
            "  inflating: computers.yml           \n",
            "  inflating: emotion.yml             \n",
            "  inflating: food.yml                \n",
            "  inflating: gossip.yml              \n",
            "  inflating: greetings.yml           \n",
            "  inflating: health.yml              \n",
            "  inflating: history.yml             \n",
            "  inflating: humor.yml               \n",
            "  inflating: literature.yml          \n",
            "  inflating: money.yml               \n",
            "  inflating: movies.yml              \n",
            "  inflating: politics.yml            \n",
            "  inflating: psychology.yml          \n",
            "  inflating: science.yml             \n",
            "  inflating: sports.yml              \n",
            "  inflating: trivia.yml              \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/chatterbotenglish.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXTylnOUUz8j",
        "outputId": "596cc3e5-489f-4f50-f590-cd2ab7a3e058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-03 12:11:58--  https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip [following]\n",
            "--2024-03-03 12:11:58--  https://github.com/shubham0204/Dataset_Archives/raw/master/chatbot_nlp.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip [following]\n",
            "--2024-03-03 12:11:58--  https://raw.githubusercontent.com/shubham0204/Dataset_Archives/master/chatbot_nlp.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24292 (24K) [application/zip]\n",
            "Saving to: ‘chatbot_nlp.zip’\n",
            "\n",
            "chatbot_nlp.zip     100%[===================>]  23.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-03 12:11:58 (70.4 MB/s) - ‘chatbot_nlp.zip’ saved [24292/24292]\n",
            "\n",
            "Archive:  chatbot_nlp.zip\n",
            "replace chatbot_nlp/data/ai.yml? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: chatbot_nlp/data/ai.yml  \n",
            "  inflating: chatbot_nlp/data/botprofile.yml  \n",
            "  inflating: chatbot_nlp/data/computers.yml  \n",
            "  inflating: chatbot_nlp/data/emotion.yml  \n",
            "  inflating: chatbot_nlp/data/food.yml  \n",
            "  inflating: chatbot_nlp/data/gossip.yml  \n",
            "  inflating: chatbot_nlp/data/greetings.yml  \n",
            "  inflating: chatbot_nlp/data/health.yml  \n",
            "  inflating: chatbot_nlp/data/history.yml  \n",
            "  inflating: chatbot_nlp/data/humor.yml  \n",
            "  inflating: chatbot_nlp/data/literature.yml  \n",
            "  inflating: chatbot_nlp/data/money.yml  \n",
            "  inflating: chatbot_nlp/data/movies.yml  \n",
            "  inflating: chatbot_nlp/data/politics.yml  \n",
            "  inflating: chatbot_nlp/data/psychology.yml  \n",
            "  inflating: chatbot_nlp/data/science.yml  \n",
            "  inflating: chatbot_nlp/data/sports.yml  \n",
            "  inflating: chatbot_nlp/data/trivia.yml  \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true -O chatbot_nlp.zip\n",
        "!unzip chatbot_nlp.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTOwnZw7U3de"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfyf4GszU8eW"
      },
      "outputs": [],
      "source": [
        "\n",
        "dir_path = '/content/chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDso9T4pU-vL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9bf035-44d8-4035-e3ee-09296a8b26c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1893\n"
          ]
        }
      ],
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( ' ' + answers_with_tags[i] + ' ' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLgcMMt8VMai"
      },
      "outputs": [],
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzizlUbQVOtA"
      },
      "outputs": [],
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIVlXV0FVRvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f94b567-f7a0-45c6-9318-79877066ff46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 22) 22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQJgporUVVtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df179b7-db9b-4a60-f614-9f87634aef47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 72) 72\n"
          ]
        }
      ],
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY7CF17dVYMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c60dd0-ba29-4e97-ecca-ad491fd9ed7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 72, 1893)\n"
          ]
        }
      ],
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlCzqpUtVZ6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61ba7b3-ea9b-4112-fd62-114e980ff8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_21 (InputLayer)       [(None, 22)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)       [(None, 72)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)     (None, 22, 200)              378600    ['input_21[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)     (None, 72, 200)              378600    ['input_22[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)               [(None, 200),                320800    ['embedding_4[0][0]']         \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               [(None, 72, 200),            320800    ['embedding_5[0][0]',         \n",
            "                              (None, 200),                           'lstm_4[0][1]',              \n",
            "                              (None, 200)]                           'lstm_4[0][2]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 72, 1893)             380493    ['lstm_5[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1779293 (6.79 MB)\n",
            "Trainable params: 1779293 (6.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax )\n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBwh4SWBVdjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c0129b-207b-42e9-bbc6-a8fb26a38746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 58s 527ms/step - loss: 7.5307\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 7s 533ms/step - loss: 7.0860\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 7s 578ms/step - loss: 6.2466\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 7s 516ms/step - loss: 6.0490\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 7s 606ms/step - loss: 5.9718\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 6s 504ms/step - loss: 5.9475\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 7s 596ms/step - loss: 5.9315\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 6s 499ms/step - loss: 5.9215\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 7s 597ms/step - loss: 5.9046\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 6s 513ms/step - loss: 5.8952\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 7s 588ms/step - loss: 5.8874\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 6s 489ms/step - loss: 5.8816\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 7s 571ms/step - loss: 5.8677\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 5.8504\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 7s 582ms/step - loss: 5.8398\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 5.8409\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 7s 590ms/step - loss: 5.8214\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 5.8201\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 7s 581ms/step - loss: 5.8063\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 6s 491ms/step - loss: 5.7963\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 7s 581ms/step - loss: 5.7896\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 6s 468ms/step - loss: 5.7747\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 7s 565ms/step - loss: 5.7643\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 6s 487ms/step - loss: 5.7526\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 7s 576ms/step - loss: 5.7396\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 6s 495ms/step - loss: 5.7288\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 7s 551ms/step - loss: 5.7066\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 6s 506ms/step - loss: 5.6997\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 7s 535ms/step - loss: 5.6805\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 6s 528ms/step - loss: 5.6745\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 6s 505ms/step - loss: 5.6552\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 6s 516ms/step - loss: 5.6393\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 7s 518ms/step - loss: 5.6234\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 7s 555ms/step - loss: 5.6049\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 6s 505ms/step - loss: 5.5830\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 7s 549ms/step - loss: 5.5669\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 6s 478ms/step - loss: 5.5430\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 7s 569ms/step - loss: 5.5132\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 6s 479ms/step - loss: 5.4880\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 7s 579ms/step - loss: 5.4627\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 6s 487ms/step - loss: 5.4351\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 7s 581ms/step - loss: 5.4222\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 6s 473ms/step - loss: 5.3857\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 7s 580ms/step - loss: 5.3510\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 6s 483ms/step - loss: 5.3313\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 7s 585ms/step - loss: 5.2994\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 6s 486ms/step - loss: 5.2559\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 7s 566ms/step - loss: 5.2366\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 6s 484ms/step - loss: 5.2179\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 7s 587ms/step - loss: 5.1834\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 5.1713\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 7s 586ms/step - loss: 5.1193\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 6s 479ms/step - loss: 5.1042\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 7s 604ms/step - loss: 5.0845\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 6s 499ms/step - loss: 5.0625\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 7s 586ms/step - loss: 5.0309\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 7s 544ms/step - loss: 4.9988\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 8s 605ms/step - loss: 4.9929\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 6s 498ms/step - loss: 4.9503\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 7s 592ms/step - loss: 4.9224\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 6s 499ms/step - loss: 4.8949\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 7s 554ms/step - loss: 4.8648\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 6s 521ms/step - loss: 4.8525\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 7s 527ms/step - loss: 4.8169\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 6s 524ms/step - loss: 4.8074\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 7s 531ms/step - loss: 4.7632\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 7s 561ms/step - loss: 4.7572\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 6s 496ms/step - loss: 4.7367\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 7s 594ms/step - loss: 4.7021\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 6s 489ms/step - loss: 4.6755\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 7s 602ms/step - loss: 4.6439\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 6s 496ms/step - loss: 4.6346\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 7s 594ms/step - loss: 4.6026\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 6s 489ms/step - loss: 4.5842\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 7s 578ms/step - loss: 4.5514\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 6s 477ms/step - loss: 4.5253\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 7s 572ms/step - loss: 4.5145\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 6s 498ms/step - loss: 4.4821\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 7s 582ms/step - loss: 4.4649\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 6s 498ms/step - loss: 4.4413\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 7s 568ms/step - loss: 4.4275\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 6s 497ms/step - loss: 4.3997\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 7s 583ms/step - loss: 4.3684\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 6s 490ms/step - loss: 4.3584\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 7s 581ms/step - loss: 4.3422\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 6s 495ms/step - loss: 4.3006\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 7s 601ms/step - loss: 4.2855\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 6s 504ms/step - loss: 4.2640\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 7s 598ms/step - loss: 4.2463\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 6s 491ms/step - loss: 4.2034\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 7s 571ms/step - loss: 4.2104\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 6s 493ms/step - loss: 4.1709\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 7s 569ms/step - loss: 4.1562\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 6s 521ms/step - loss: 4.1367\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 7s 519ms/step - loss: 4.1222\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 6s 506ms/step - loss: 4.1012\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 7s 543ms/step - loss: 4.0906\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 7s 580ms/step - loss: 4.0432\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 6s 501ms/step - loss: 4.0301\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 7s 595ms/step - loss: 4.0086\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 6s 499ms/step - loss: 3.9817\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 7s 599ms/step - loss: 3.9703\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 6s 506ms/step - loss: 3.9447\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 7s 609ms/step - loss: 3.9337\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 6s 496ms/step - loss: 3.9158\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 7s 594ms/step - loss: 3.8811\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 6s 502ms/step - loss: 3.8703\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 7s 602ms/step - loss: 3.8464\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 6s 508ms/step - loss: 3.8193\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 7s 591ms/step - loss: 3.8073\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 6s 494ms/step - loss: 3.7900\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 7s 598ms/step - loss: 3.7551\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 3.7440\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 7s 561ms/step - loss: 3.7289\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 6s 513ms/step - loss: 3.7077\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 7s 552ms/step - loss: 3.6862\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 6s 535ms/step - loss: 3.6592\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 7s 523ms/step - loss: 3.6527\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 7s 589ms/step - loss: 3.6255\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 6s 502ms/step - loss: 3.6175\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 7s 596ms/step - loss: 3.5924\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 3.5745\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 7s 582ms/step - loss: 3.5474\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 6s 478ms/step - loss: 3.5319\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 7s 596ms/step - loss: 3.4989\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 6s 489ms/step - loss: 3.4881\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 7s 586ms/step - loss: 3.4695\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 6s 499ms/step - loss: 3.4528\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 7s 597ms/step - loss: 3.4360\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 6s 505ms/step - loss: 3.4224\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 7s 594ms/step - loss: 3.3907\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 6s 510ms/step - loss: 3.3708\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 7s 594ms/step - loss: 3.3519\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 6s 506ms/step - loss: 3.3398\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 7s 587ms/step - loss: 3.3181\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 6s 497ms/step - loss: 3.3018\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 7s 564ms/step - loss: 3.2800\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 6s 484ms/step - loss: 3.2613\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 7s 574ms/step - loss: 3.2433\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 6s 510ms/step - loss: 3.2170\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 7s 535ms/step - loss: 3.2045\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 6s 506ms/step - loss: 3.1911\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 7s 530ms/step - loss: 3.1677\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 6s 533ms/step - loss: 3.1495\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 7s 518ms/step - loss: 3.1255\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 7s 549ms/step - loss: 3.1127\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 6s 490ms/step - loss: 3.1026\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 7s 584ms/step - loss: 3.0841\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 6s 487ms/step - loss: 3.0664\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 7s 585ms/step - loss: 3.0324\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 )\n",
        "model.save( 'model.keras' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdNFRJ2IaIMM"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIexrl-dVgWq"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] )\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAkp-DpnDzrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOfnXEYS_Nd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChhgjhS9WJ3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9e49a70b-272e-473a-c851-cc88caa0233a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-49e00dc674a4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ]\n",
        "\n",
        "    print( decoded_translation )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}